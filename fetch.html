<!doctype html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Image Gallery" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="styles.css" />
    <title>Fetch Robot</title>
</head>

<body class="dark">
    <header class="header center">
        <h3>
            <a href="index.html" class="link">Back to Portfolio</a>
        </h3>
    </header>

    <main>
        <section id="gallery" class="section gallery">
            <h2 class="section__title" style="margin: 1em; margin-top: 0em;">Autonomous AI Fetch Robot</h2>
            <p style="text-align: left; padding-bottom: 2em; line-height: 1.5; max-width: 1080px; margin: 0 auto;"> This personal project involved designing and building a car robot equipped with a Raspberry Pi, a camera, IR, and ultrasonic sensors.<br><br>
            The goal was to create a robot capable of detecting and following a person using OpenCV for body and ball contour
            recognition, picking up a dropped ball, and returning it. The project was made more challenging by the constraint of a lack of absolute positioning sensors, such as gyroscope or IMUs, or LIDAR. The design required integrating the frame, connecting electrical
            components, and overcoming challenges related to sensor coordination and object detection.
            </p>
            <div class="gallery__grid">
                <iframe width="1280" height="720" src="https://www.youtube.com/embed/3hdKIE38dDQ">
                </iframe>
            </div>

            <h2 class="section__title" style="margin: 1em; margin-top: 2em;">Process and methodology</h2>
            <p style="text-align: left; padding-bottom: 2em; line-height: 1.5; max-width: 1080px; margin: 0 auto;">
                <img src="Fetch_Robot_Transparent.png" alt="Fetch Robot" style="float: left; margin: -2em; margin-left: -5em; max-width: 50%;">
                The development of the Autonomous AI Fetch Robot began with designing a lightweight and stable laser-cut acrylic frame to house the
                electrical components, ensuring accessibility for wiring and adjustments.<br><br>The robot was equipped with a Raspberry Pi for
                computation, a camera for vision processing, and IR and ultrasonic sensors for object detection and obstacle avoidance.
                Using Python and OpenCV, the robot’s computer vision capabilities were implemented to recognize body and ball contours.<br><br>
                The integration of sensors required precise calibration to ensure accurate distance measurements and reliable object
                detection in varying environments. Testing and iterative refinement were critical to optimizing the robot’s ability to
                detect and follow a moving person, locate and retrieve a dropped ball, and return it successfully.
                
            </p>

            <h2 class="section__title" style="margin: 1em; margin-top: 0em;">Reasoning and reflection</h2>
            <p style="text-align: left; padding-bottom: 2em; line-height: 1.5; max-width: 1080px; margin: 0 auto;">
                <img src="OpenCV_1.png" alt="OpenCV Original Image"
                    style="float: right; margin: 0em; margin-left: 4em; margin-right: 2em; max-width: 30%;">
                The absence of absolute positioning sensors such as gyroscopes or LIDAR posed a significant challenge, requiring
                creative problem-solving to ensure the robot’s functionality. By leveraging the camera for vision-based navigation and
                combining sensor data from IR and ultrasonic components, a robust and cost-effective alternative was developed.<br><br>The
                project highlighted the importance of balancing hardware constraints with software solutions, such as compensating for
                sensor limitations through advanced image processing techniques. Reflecting on the project, the iterative process of
                testing and debugging reinforced the value of adaptability and persistence in addressing technical challenges, while
                deepening my understanding of sensor integration and autonomous robotics.
                
                <br><br>Next steps: Implement SLAM using LIDAR, mapping surroundings to enable point-to-point navigation.</p>
    </main>

    <script src="./script.js"></script>
</body>

</html>